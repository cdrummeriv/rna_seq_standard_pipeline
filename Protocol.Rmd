# Introduction
This pipeline is for bulk RNAseq analysis using snakemake workflow.  This work is forked from [khayer/rna_seq_standard_pipeline](https://github.com/khayer/rna_seq_standard_pipeline).  This fork is meant to be a detailed, step-by-step, protocol for traditionally trained life scientists with beginner level to intermidiate level bioinformatics training.  I suggest investing in a copy of "Bioinformatics Data Skills" by Vince Buffalo and reading (or refer to) chapters 1-7.  My README files are written in .Rmd using RStudio (I use this file format and style becuase its how I was introduced to bioinformatics and I like the collapsable outline structure).  This page will use publically available data (Geo dataset [GSE131473](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=gse131473)).

## Pipeline Overview
This pipeline takes fastq files and feeds them into the following pipeline/ rulegraph.
![example dag](misc/dag.svg)
- 'trimming': removes known adaptor sequences reads
- 'fastqc': quality control examining base calls
- 'align': aligns fastq files to reference genome using the [STAR aligner](https://github.com/alexdobin/STAR)
	- 'bamCoverage_CPM':
	- 'run_regtools': takes aligned files and takes out junctions (STAR can perform its version of this step with the aligner
	- 'run_TPMCalculator': generates TPM from BAM files
- 'salmon': "transcript level" analysis (as apposed to STAR aligner's genome level transcript analysis).  Note: short reads data analysis can skew 'salmon' results.  Long reads prefered for this pipeline

## Folder Structure
For this snakemake pipeline, raw data and project data can be stored with the following directory structure:
ATT Lab Root Directory/
```{folder structure}
├── mnt/ipsilon/thomas-tikhonenko_lab/data/public_data/
|	├── current_project_title/
|		├── SRR_Acc_List.txt
|		├── **labfastqFilesRaw**
├── home/username/
|	├── data/
|		├── current_project_title/
|			├── SRR_Acc_List.txt
|			├── **labfastqFilesRaw**
|	├── projects/
|		├── current_project_title/
|			├── reads/ -> fastqFilesLink
|			├── config/
|				├──SraRunTable.csv
|				├── config.yaml
├── scr1/users/username
|	├── temp_data/
|		├── current_project_title/
|			├── **publicFastqFilesRaw**
```
Create a new project in the project directory
```{creating new project template}
# cd to desired directory 
$ mkdir -p project_name/{reads,config}; ls -Rl
```
## Conda Environment
This pipeline requires conda environments
```{installing conda environment and other dependencies}
# navigate to home directory, download and install
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -O ~/miniconda.sh
bash ~/miniconda.sh -b -p $HOME/miniconda

# install conda with Homebrew
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
brew install --cask anaconda

# create conda environment
conda update conda # update conda if already installed
conda install -n base -c conda-forge mamba #install mamba (alternative package manager to conda)
mamba create -c conda-forge -c bioconda -n snakemake snakemake # this step also creates a "snakemake" environment
conda upgrade -c bioconda -c conda-forge snakemake # ensure snakemake is up to date
conda install -c bioconda snakemake-wrapper-utils # install add-on required for pipeline
conda activate snakemake # activate conda environment
```
# Cluster Processing Configuration (First Run Only)
CHOP using 'slurm' for cluster processing.  Once our snakemake is created, you need "attach" slurm to the environment.
```{Creating config file for cluster use in snakemake environment}
# return to home directory, find .congif directory, search for a snakemake folder
$ cd; cd .config; ls

# if no .config/snakemake folder exists, create the following directory path, create then edit a config.yaml in that location
$ touch .config/snakemake/slurm/config.yaml
$ sublime .config/snakemake/slurm/config.yaml
```
Add the following to the config file
```{.config/snakemake/config.yaml file content}
jobs: 500
cluster: "sbatch -t {resources.time} --mem={resources.mem}G -c {resources.cpu} -o logs_slurm/{rule}_{wildcards}.o -e logs_slurm/{rule}_{wildcards}.e"
default-resources: [ mem=2000, time=60, cpu=1]
```
Save then close and verify content in the terminal:
```{ verify config.yaml file content}
$ head .config/snakemake/config.yaml
```
Common 'slurm' commands to check on submitted jobs
```{Common slurm commands}
$ squeue --me -t R # List my running jobs
$ squeue --me # List all my jobs
$ sacct -j <jobid> -l --units=G # Qacct equivalent with memory output as GB
$ sacct -j <jobid> -o maxrss # Show max memory used by job
```
# Staging Files for Pipeline: Metadata, FASTQ and Config Files
FASTQ and associated metadata files are required to run the pipeline.  Publicly available RNAseq data can be found on [NCBI's Gene Expression Omnibus](https://www.ncbi.nlm.nih.gov/geo/). From a study's Accession Display page, select "SRA Run Selector" under the "Supplementary File" section.
## Metadata Files: SraRunTable and SRR_Acc_List
Under the "Select" section, download both the Metadata (SraRunTable.txt) and the Accession List (SRR_Acc_List.txt).
### Create Sample Spreadsheet (from SraRunTable.txt)
Create a "sample spreadsheet" from the SraRunTable.txt file:
- Open in Excel the select "File > Open" and find the SraRunTable.txt file
- Follow the Import Wizard directions (Delimited > Comma > Finish)

The final document must have:

![example spreadsheet](misc/example_sample_table.png)
- **"Run" column**: the unique read ID.  Typically downloaded as SRR.
  - This version of the pipeline requires forward and reverse reads to be separate fastq files.   (GSM#_1 and GSM#_2).  Rename files if necessary and update these changes in the SRR_Acc_List.
- **"sample_name" column**: a description of the samples group, treatment, replicate.
- **"group" column**: the group information only (specifying test vs control)
- be saved as '.csv' file in the project's config folder

Note: "Run", "sample_name" and "group" columns must be included in the table, however they do not need to be in this specific order.  Additional columns can also be added.

### Create Sample List (from SRR_Acc_List.txt)
The SRR list will be used to automate downloading FASTQ files.  Move the SRR_Acc_List.txt to the project's run directory
## FASTQ Files
"Raw data" fastq files should be stored separately from the "working copy" used in the project folder.  
### FASTQ Raw Data
Download **raw data** to either:
- **ATT shared directory**: for experimental data generated by the lab
- **User directory**: for experimental data generated by the lab
- **'/scr/users/username'**: for publicly available datasets
Note: these drives may have storage limits, contact on-staff bioinformaticians for specifics.

Create a conda environment and install SRA tools
```{sra_tools enviornoment setup}
# return to home directory and create a SRA tools environment
cd; conda create -n SRA_Tools; conda env list # semicolon allows code to be run subsequently.  Each section delimited by semicolons can be run on its own line as well
conda activate SRA_Tools; conda install -c bioconda sra-tools # activate SRA environment and install using conda

# download and install SRA Tools
curl -OL --output sratoolkit.tar.gz http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-mac64.tar.gz
tar -vxzf sratoolkit.tar.gz
# append the path to the binaries to your PATH environment variable (the format "sratoolkit.<release>-<platform> varies):
export PATH=$PATH:$PWD/sratoolkit.2.11.2-mac64/bin

# Configure SRA tools
vdb-config -i
X # enables remote access

# Move the Accession List (SRR_Acc_List.txt) to directory to store the fastq files
$ cd; cd /scr1/users/drummerc; ls # navigates to home, then to project's read folder and verifies content

# Create temp folders if required
$ mkdir -p temp_data/gse131473
```
Publicly available fastq files can be downloaded automatically:
#### Cluster Download:
```{cluster fastq download with SRA Tools}
# Generate SRA tools commands to download fastq files for each sample listed in the SRR Acc List, save lines of code in a shell script, then runs script
$ cat SRR_Acc_List.txt | xargs -i{} echo "sbatch -J fastq_{} --wrap=\"fastq-dump --gzip --split-3 {}\"" > download.sh
$ bash download.sh # shell script submits download jobs to cluster using "slurm"
```
#### Local download:
```{local fastq download with SRA Tools}
# Change to desired directory for download,
## Download a single SRR from
fasterq-dump --split-files SRR11180057

# download a list of SRR accession numbers from
$ cat SRR_Acc_List.txt | xargs -I{} echo "fastq-dump --gzip --split-3 {}" > download.sh
$ bash download.sh # shell script to download files
```
If files need to be renamed, refer to the "rename" tab of the GSE131473_summary.xlsx.

#### FASTQ Working Copy
Make a "working copy" by creating soft links to the raw fastq files in the project's 'read' directory.
```{soft link for fastq working copy}
# Navigate reads folder, create soft links to original fastq files
$ cd; cd projects/gse131473/reads; ls
$ ln -s /scr1/users/drummerc/temp_data/gse131473/*fastq.gz .; ls # creates soft link of files ending with "fastq.qz" from the scr1/... temp folder to the current location " .".
```
### Snakemake 'config.yaml' Configuration File
A snakemake 'config.yaml' file needs to be located in the project config folder. Note: this is a separate config.yaml file from before.

Create a config file:
```{Creating config.yaml file}
# navigate to config folder
$ cd; cd projects/gse131473/config

# create a blank config file then open in sublime text to edit
$ touch config.yaml; sublime config.yaml # "sublime" is an alias (variable or an "alias" containing specific code)
```
This configuration file contains information about the specific reference material for this projects snakemake run.
#### .yaml Config for Cluster and Local Runs
```{config.yaml file content}
# Relative paths
samples: "SraRunTable_fraction_rna.csv"
comparison: "config/comparison.csv"
comparisons: "metadata/comparisons.csv"

# Cluster Runs
star_index: "/mnt/isilon/thomas-tikhonenko_lab/data/index/GRCh38.p13/star.2.7.3a_gencode.v37_GRCh38.13" # "STAR_genome" in Manu's code
salmon_index: "/mnt/isilon/thomas-tikhonenko_lab/data/gencode/gencode.v30.transcripts_salmon" # "salmon_genome" in Manu's code
blacklist: "/mnt/isilon/thomas-tikhonenko_lab/data/hg38-blacklist.v2.bed"
gff3: "/mnt/isilon/thomas-tikhonenko_lab/data/index/GRCh38.p13/gencode.v32.primary_assembly.annotation.gff3"
gtf: "/mnt/isilon/thomas-tikhonenko_lab/data/index/GRCh38.p13/gencode.v32.primary_assembly.annotation.gtf"
selected_genes: "/mnt/isilon/thomas-tikhonenko_lab/data/index/selected_genes.bed"
tmp_dir: "/scr1/users/drummerc/tmp"
single_end: False
stranded: False

# Local Runs
star_index: "/Volumes/Bioinfo/index/GRCh38.p13/GENCODE_GRCh38.p13" # "STAR_genome" in Manu's code
salmon_index: "/Volumes/Bioinfo/index/gencode/gencode.v30.transcripts_salmon" # "salmon_genome" in Manu's gencode
blacklist: "/Volumes/Bioinfo/index/hg38-blacklist.v2.bed"
gff3: "/Volumes/Bioinfo/index/GRCh38.p13/gencode.v32.primary_assembly.annotation.gff3"
gtf: "/Volumes/Bioinfo/index/GRCh38.p13/gencode.v32.primary_assembly.annotation.gtf"
tmp_dir: "/scr1/users/drummerc/tmp"
selected_genes: "/mnt/isilon/thomas-tikhonenko_lab/data/index/selected_genes.bed"
single_end: True
stranded: False
```
The config file should hold the following fields (this example is for hg38, but can be replaced for any other version/ species):
- samples: the SRA Table metadata file downloaded from SRA Run Selector
- star_index:
- salmon_index:
- blacklist:
- gtf:
- gff3:
- tmp_dir: (will have to create before first run)
- single_end: are the fastq files R1 and R2 or just a single R1 (should be able to tell from SRA Run Selector)
- stranded: whether the reads are forward (+) and reverse (-) strand information

Note: most of these files are in the shared ATT lab drive.  Access files via the soft link in your user home directory created earlier.

# Running Pipeline
## Dry Run Testing
Navigate to the project directory to begin. The first argument for slurm is the location of the 'Snakefile' in the pipeline directory. The '--configfile' tag must point to the sample/annotation config file located in the project/config directory.

Check parameters using '--dryrun' or '-n' tag before submitting the final job to the cluster.
```{Snakemake dryrun}
# Local dryrun
$ snakemake -s ~/tools/rna_seq_standard_pipeline/workflow/Snakefile -p --use-conda --configfile config/config.yaml -n


# cluster dryrun
$ snakemake --profile slurm -s ~/tools/rna_seq_standard_pipeline/workflow/Snakefile -p --use-conda --configfile config/config.yaml -n
```


snakemake -s /home/drummerc/tools/rna_seq_standard_pipeline/workflow/Snakefile -p --use-conda --configfile config/config.yaml -n
